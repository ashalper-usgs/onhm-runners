{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When docker chokes on containers that have worked before, try clearing out disk space:\n",
    "(from https://stackoverflow.com/questions/30719896/docker-dm-task-run-failed-error)\n",
    "\n",
    "sudo service docker stop\n",
    "sudo thin_check /var/lib/docker/devicemapper/devicemapper/metadata\n",
    "sudo thin_check --clear-needs-check-flag /var/lib/docker/devicemapper/devicemapper/metadata\n",
    "sudo service docker start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and set some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import ftplib\n",
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "base_dir = '/work/markstro/operat/setup/test/'\n",
    "work_dir = base_dir + 'NHM-PRMS_CONUS'\n",
    "ftp_server = 'ftpint.usgs.gov'\n",
    "ftp_path = 'pub/cr/co/denver/BRR-CR/pub/markstro/onhm/'\n",
    "ftp_login = 'anonymous'\n",
    "ftp_pw = ''\n",
    "python_module_path = \"/work/markstro/operat/repos/onhm-runners/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the NHM-PRMS workspace directory and subdirectories. This downloads param file, control file, etc. It creates the PRMS workspace for the ONHM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the old work space\n",
    "if os.path.exists(work_dir):\n",
    "    shutil.rmtree(work_dir)\n",
    "    \n",
    "os.chdir(base_dir)\n",
    "\n",
    "filename = 'NHM-PRMS_CONUS.tar.gz'\n",
    "\n",
    "# Get the tar ball from ftp\n",
    "ftp = ftplib.FTP(ftp_server) \n",
    "ftp.login(ftp_login, ftp_pw) \n",
    "ftp.cwd(ftp_path)\n",
    "ftp.retrbinary(\"RETR \" + filename, open(filename, 'wb').write)\n",
    "ftp.quit()\n",
    "\n",
    "# unzip and untar\n",
    "tf = tarfile.open(base_dir + filename)\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install thredds server into NHM_PRMS workspace\n",
    "\n",
    "\n",
    "Not implemented yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the historical PRMS run offline. There are still lots of issues (ie, updats to the HRUs and the current CBH files end in 2014), but goto\n",
    "\n",
    "/work/markstro/operat/setup/init_file/NHM-PRMS_CONUS\n",
    "\n",
    "to run \"run_for_init.sh\" that will create the prms_save_XXXX_XX_XX.init file.\n",
    "\n",
    "Right now, download an init file from the ftp server to illustrate that the initial init file comes from outside of what is going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(work_dir)\n",
    "\n",
    "filename = 'prms_save_2019-03-21.init'\n",
    "\n",
    "# Get the init file from ftp\n",
    "ftp = ftplib.FTP(ftp_server) \n",
    "ftp.login(ftp_login, ftp_pw) \n",
    "ftp.cwd(ftp_path)\n",
    "ftp.retrbinary(\"RETR \" + filename, open(filename, 'wb').write)\n",
    "ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "Here starts the operational loop. The following steps are repeated everyday.\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_start_date = 2019-03-31\n",
      "model_end_date = 2019-05-30\n",
      "fetch_start_date = 2019-03-30\n",
      "fetch_end_date = 2019-05-30\n",
      "init_start_date = 2019-03-21\n",
      "init_end_date = 2019-03-31\n"
     ]
    }
   ],
   "source": [
    "yesterday = datetime.today() - timedelta(1)\n",
    "sixtydays = yesterday - timedelta(60)\n",
    "initdate = sixtydays - timedelta(1)\n",
    "\n",
    "model_start_date = sixtydays\n",
    "model_end_date = yesterday\n",
    "\n",
    "fetch_start_date = initdate\n",
    "fetch_end_date = yesterday\n",
    "\n",
    "# The init_start_date is one day after whatever the current init_date is. Get this from the\n",
    "# file in the workspace.\n",
    "os.chdir(work_dir)\n",
    "for file in glob.glob(\"*.init\"):\n",
    "    ;\n",
    "    \n",
    "tok = file.split('_')\n",
    "foo = tok[2]\n",
    "tok = foo.split('.')\n",
    "date_str = tok[0]\n",
    "\n",
    "init_start_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "init_end_date = sixtydays\n",
    "\n",
    "print('model_start_date = ' + model_start_date.strftime('%Y-%m-%d'))\n",
    "print('model_end_date = ' + model_end_date.strftime('%Y-%m-%d'))\n",
    "print('fetch_start_date = ' + fetch_start_date.strftime('%Y-%m-%d'))\n",
    "print('fetch_end_date = ' + fetch_end_date.strftime('%Y-%m-%d'))\n",
    "print('init_start_date = ' + init_start_date.strftime('%Y-%m-%d'))\n",
    "print('init_end_date = ' + init_end_date.strftime('%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Fetcher-Parser to bring the CBH netcdf up to yesterday. This copy from stage business simulates that Fetcher-Parser.\n",
    "\n",
    "Not implemented yet, but pull from ftp as a place holder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(work_dir)\n",
    "\n",
    "filename = 'new.nc'\n",
    "\n",
    "# Get the init file from ftp\n",
    "ftp = ftplib.FTP(ftp_server) \n",
    "ftp.login(ftp_login, ftp_pw) \n",
    "ftp.cwd(ftp_path)\n",
    "ftp.retrbinary(\"RETR \" + filename, open(filename, 'wb').write)\n",
    "ftp.quit()\n",
    "\n",
    "# shutil.copy(stage_dir + 'stage_2/new.nc', work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the container for the Fetcher-Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['docker', 'pull', 'nhmusgs/ofp']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2ee5c4b9a32b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0marg_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need this for whatever reason; don't move arg.split into the func call below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/markstro/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 395\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/markstro/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 487\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['docker', 'pull', 'nhmusgs/ofp']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "os.chdir(work_dir)\n",
    "\n",
    "# docker pull for ofp\n",
    "arg = 'docker pull nhmusgs/ofp'\n",
    "arg_l = arg.split() # need this for whatever reason; don't move arg.split into the func call below\n",
    "\n",
    "subprocess.check_output(arg_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg = 'docker run --rm -v ' + work_dir + ':/work nhmusgs/prms:5.0.0 ' + fa + sa + ea + ia\n",
    "# arg_l = arg.split() # need this for whatever reason; don't move arg.split into the func call below\n",
    "\n",
    "# subprocess.check_output(arg_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run from onhm-runners ncf2cbh.ncf2cbh.py. This will write PRMS format CBH files that contain\n",
    "data from 2019-01-01 up through yesterday.\n",
    "\n",
    "It's here, but needs more work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting dir = /work/markstro/operat/setup/test/NHM-PRMS_CONUS/\n",
      "base_date_str 2019-05-13 00:00:00\n",
      "2019-05-13\n",
      "writing /work/markstro/operat/setup/test/NHM-PRMS_CONUS/prcp.cbh\n",
      "writing /work/markstro/operat/setup/test/NHM-PRMS_CONUS/tmax.cbh\n",
      "writing /work/markstro/operat/setup/test/NHM-PRMS_CONUS/tmin.cbh\n"
     ]
    }
   ],
   "source": [
    "# Not sure about the best way to handle importation of modules in jupyter,\n",
    "# but this seems to work\n",
    "# My PRMS python utilities are in onhm-runners/prms_utils\n",
    "if python_module_path not in sys.path:\n",
    "    sys.path.append(python_module_path)\n",
    "\n",
    "# Spell out full paths -- don't use string variables -- don't know why.\n",
    "%run '/work/markstro/operat/repos/onhm-runners/ncf2cbh/ncf2cbh.py' '/work/markstro/operat/setup/test/NHM-PRMS_CONUS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PRMS-NHM in the workspace. This run should always go from the day after the current\n",
    "init file up through yesterday.\n",
    "\n",
    "The next step works to run PRMS, but the start_time and end_time logic is not figured out yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -set start_date 2019,3,31,0,0,0\n",
      " -set end_date 2019,5,30,0,0,0\n",
      " -set var_init_file prms_save_2019-03-30.init\n",
      "docker run --rm -v /work/markstro/operat/setup/test/NHM-PRMS_CONUS:/work nhmusgs/prms:5.0.0 ./NHM-PRMS.control -set init_vars_from_file 1 -set save_vars_to_file 0 -set start_date 2019,3,31,0,0,0 -set end_date 2019,5,30,0,0,0 -set var_init_file prms_save_2019-03-30.init\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\\n\\nWARNING: nhruOut_format is duplicated in the control file ./NHM-PRMS.control.\\n\\n\\n\\n\\n                         U.S. Geological Survey\\n               Precipitation-Runoff Modeling System (PRMS)\\n                        Version 5.0.0 05/10/2019\\n\\n        Process            Available Modules\\n--------------------------------------------------------------------\\n  Basin Definition: basin\\n    Cascading Flow: cascade\\n  Time Series Data: obs, water_use_read, dynamic_param_read\\n   Potet Solar Rad: soltab\\n  Temperature Dist: temp_1sta, temp_laps, temp_dist2, climate_hru\\n       Precip Dist: precip_1sta, precip_laps, precip_dist2,\\n                    climate_hru\\nTemp & Precip Dist: xyz_dist, ide_dist\\n    Solar Rad Dist: ccsolrad, ddsolrad, climate_hru\\nTranspiration Dist: transp_tindex, climate_hru, transp_frost\\n      Potential ET: potet_hamon, potet_jh, potet_pan, climate_hru,\\n                    potet_hs, potet_pt, potet_pm, potet_pm_sta\\n      Interception: intcp\\n     Snow Dynamics: snowcomp\\n    Surface Runoff: srunoff_smidx, srunoff_carea\\n         Soil Zone: soilzone\\n       Groundwater: gwflow\\nStreamflow Routing: strmflow, strmflow_in_out, muskingum,\\n                    muskingum_lake\\n    Output Summary: basin_sum, subbasin, map_results, prms_summary,\\n                    nhru_summary, nsub_summary, water_balance\\n                    basin_summary, nsegment_summary\\n     Preprocessing: write_climate_hru, frost_date\\n--------------------------------------------------------------------\\n\\n\\n==========================================================================\\nPlease give careful consideration to fixing all ERROR and WARNING messages\\n==========================================================================\\n\\n\\n    Active modules listed in the order in which they are called\\n\\n        Process                   Module                Version Date\\n====================================================================\\nComputation Order              call_modules               2019-05-10\\nBasin Definition               basin                      2018-04-06\\nCommon States and Fluxes       climateflow                2019-04-03\\nPotential Solar Radiation      soltab                     2016-09-09\\nParameter Setup                setup_param                2016-11-28\\nPRMS Set Time Variables        prms_time                  2015-03-31\\nTime Series Data               obs                        2018-02-23\\nTemperature Distribution       climate_hru                2017-10-23\\nPrecipitation Distribution     climate_hru                2017-10-23\\nSolar Radiation Distribution   ddsolrad                   2019-04-04\\nTranspiration Distribution     transp_tindex              2015-01-06\\nPotential Evapotranspiration   potet_jh                   2019-04-04\\nCanopy Interception            intcp                      2018-02-26\\nSnow Dynamics                  snowcomp                   2018-05-04\\nSurface Runoff                 srunoff_smidx              2019-04-03\\nSoil Zone Computations         soilzone                   2019-03-05\\nGroundwater                    gwflow                     2019-04-04\\nRouting Initialization         routing                    2018-04-25\\nStreamflow Routing             muskingum                  2017-10-06\\nSummary                        basin_sum                  2017-10-21\\nNhru Output Summary            nhru_summary               2018-06-11\\nNsegment Output Summary        nsegment_summary           2018-06-11\\n====================================================================\\n\\nWriting PRMS Water Budget File: ./output/NHM-PRMS.out\\n\\nSimulation time period: 1980/10/01 - 1981/09/30\\n\\n====================================================================\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(work_dir)\n",
    "#arg = 'docker run --rm -v ' + work_dir + ':/work prms:5.0.0 ./NHM-PRMS.control'\n",
    "\n",
    "fa = './NHM-PRMS.control -set init_vars_from_file 1 -set save_vars_to_file 0'\n",
    "\n",
    "# figure out the following from start_date and end_date\n",
    "sa = ' -set start_date ' + str(model_start_date.year) + ',' + str(model_start_date.month) + ',' + str(model_start_date.day) + ',0,0,0'\n",
    "print(sa)\n",
    "ea = ' -set end_date ' + str(model_end_date.year) + ',' + str(model_end_date.month) + ',' + str(model_end_date.day) + ',0,0,0'\n",
    "print(ea)\n",
    "ia = ' -set var_init_file prms_save_' + initdate.strftime('%Y-%m-%d') + '.init'\n",
    "print(ia)\n",
    "\n",
    "arg = 'docker run --rm -v ' + work_dir + ':/work nhmusgs/prms:5.0.0 ' + fa + sa + ea + ia\n",
    "print(arg)\n",
    "arg_l = arg.split() # need this for whatever reason; don't move arg.split into the func call below\n",
    "\n",
    "subprocess.check_output(arg_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine if the run completed successfully and evaluate for goodness.\n",
    "\n",
    "Here's the initial version:\n",
    "1) Check to see if the file work_dir + output/ + NHM-PRMS.out exists\n",
    "2) Check for the last line \"Execution elapese time...\" If PRMS doesn't make it all the way through the time loop, this line won't be there.3) Read the amount of time from the last line. It should be greater than (say) 1 minute. This may need to be refined.\n",
    "\n",
    "If any of these checks fail, this step should return False; otherwise, return True. This python should go into it's own container as the \"ONHM Verifier\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "fname = work_dir + '/output/NHM-PRMS.out'\n",
    "min_time = 1\n",
    "\n",
    "verified = True\n",
    "if not os.path.isfile(fname):\n",
    "    verified = False\n",
    "    \n",
    "else:\n",
    "    with open(fname, 'r') as f:\n",
    "        found = False\n",
    "        for line in f:\n",
    "            if 'Execution elapsed time' in line:\n",
    "                found = True\n",
    "                break\n",
    "                \n",
    "        if not found:\n",
    "            verified = False\n",
    "            \n",
    "        else:\n",
    "            tok = line.split()\n",
    "            mn = int(tok[3])\n",
    "            sc = float(tok[5])\n",
    "                \n",
    "            if mn < min_time:\n",
    "                verified = False\n",
    "\n",
    "\n",
    "print(verified)  # this should be the return value of the container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the current model run was good, run prms_outputs2_ncf.py to convert the output to ncf. This will eventually run in a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting dir = /work/markstro/operat/setup/test/NHM-PRMS_CONUS/\n",
      "processing soil_moist\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/work/markstro/operat/repos/onhm-runners/out2ncf/prms_outputs2_ncf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/markstro/operat/repos/onhm-runners/out2ncf/prms_outputs2_ncf.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconversion_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mval_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Not sure about the best way to handle importation of modules in jupyter,\n",
    "# but this seems to work\n",
    "# My PRMS python utilities are in onhm-runners/prms_utils\n",
    "\n",
    "if python_module_path not in sys.path:\n",
    "    sys.path.append(python_module_path)\n",
    "\n",
    "# Spell out full paths -- don't use string variables -- don't know why.\n",
    "%run '/work/markstro/operat/repos/onhm-runners/out2ncf/prms_outputs2_ncf.py' '/work/markstro/operat/setup/test/NHM-PRMS_CONUS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the current model run was good, run PRMS-NHM in the workspace to update the init file from whatever date it is at to yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(work_dir)\n",
    "\n",
    "# get the list of existing init files.\n",
    "old_init_file_name_list = []\n",
    "os.chdir(work_dir)\n",
    "for file in glob.glob(\"*.init\"):\n",
    "    old_init_file_name_list.append(file)\n",
    "    \n",
    "print(old_init_file_name_list)\n",
    "\n",
    "# Create the new init file\n",
    "\n",
    "fa = './NHM-PRMS.control -set init_vars_from_file 1 -set save_vars_to_file 1'\n",
    "\n",
    "# figure out the following from start_date and end_date\n",
    "sa = ' -set start_date ' + str(init_start_date.year) + ',' + str(init_start_date.month) + ',' + str(init_start_date.day) + ',0,0,0'\n",
    "print(sa)\n",
    "ea = ' -set end_date ' + str(init_end_date.year) + ',' + str(init_end_date.month) + ',' + str(init_end_date.day) + ',0,0,0'\n",
    "print(ea)\n",
    "\n",
    "ia = ' -set var_init_file prms_save_' + init_start_date.strftime('%Y-%m-%d') + '.init' + ' -set var_save_file prms_save_' + init_end_date.strftime('%Y-%m-%d') + '.init'\n",
    "print(ia)\n",
    "\n",
    "arg = 'docker run --rm -v ' + work_dir + ':/work nhmusgs/prms:5.0.0 ' + fa + sa + ea + ia\n",
    "arg_l = arg.split() # need this for whatever reason; don't move arg.split into the func call below\n",
    "\n",
    "subprocess.check_output(arg_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure that the PRMS init model run worked. If it did, erase the old init files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done for this day's step. Wait until tomorrow and go back to the date computation step and repeat subsequent steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
