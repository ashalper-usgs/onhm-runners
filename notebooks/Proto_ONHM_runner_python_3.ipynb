{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and set some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import ftplib\n",
    "import tarfile\n",
    "\n",
    "base_dir = '/work/markstro/operat/setup/test/'\n",
    "work_dir = base_dir + 'NHM-PRMS_CONUS'\n",
    "ftp_server = 'ftpint.usgs.gov'\n",
    "ftp_path = 'pub/cr/co/denver/BRR-CR/pub/markstro/onhm/'\n",
    "ftp_login = 'anonymous'\n",
    "ftp_pw = ''\n",
    "python_module_path = \"/work/markstro/operat/repos/onhm-runners/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the NHM-PRMS workspace directory and subdirectories. This downloads param file, control file, etc. It creates the PRMS workspace for the ONHM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the old work space\n",
    "if os.path.exists(work_dir):\n",
    "    shutil.rmtree(work_dir)\n",
    "    \n",
    "os.chdir(base_dir)\n",
    "\n",
    "filename = 'NHM-PRMS_CONUS.tar.gz'\n",
    "\n",
    "# Get the tar ball from ftp\n",
    "ftp = ftplib.FTP(ftp_server) \n",
    "ftp.login(ftp_login, ftp_pw) \n",
    "ftp.cwd(ftp_path)\n",
    "ftp.retrbinary(\"RETR \" + filename, open(filename, 'wb').write)\n",
    "ftp.quit()\n",
    "\n",
    "# unzip and untar\n",
    "tf = tarfile.open(base_dir + filename)\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install thredds server into NHM_PRMS workspace\n",
    "\n",
    "\n",
    "Not implemented yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the historical PRMS run offline. There are still lots of issues (ie, updats to the HRUs and the current CBH files end in 2014), but goto\n",
    "\n",
    "/work/markstro/operat/setup/init_file/NHM-PRMS_CONUS\n",
    "\n",
    "to run \"run_for_init.sh\" that will create the prms_save_XXXX_XX_XX.init file.\n",
    "\n",
    "Right now, download an init file from the ftp server to illustrate that the initial init file comes from outside of what is going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(work_dir)\n",
    "\n",
    "filename = 'prms_save_2019-03-21.init'\n",
    "\n",
    "# Get the init file from ftp\n",
    "ftp = ftplib.FTP(ftp_server) \n",
    "ftp.login(ftp_login, ftp_pw) \n",
    "ftp.cwd(ftp_path)\n",
    "ftp.retrbinary(\"RETR \" + filename, open(filename, 'wb').write)\n",
    "ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "Here starts the operational loop. The following steps are repeated everyday.\n",
    "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_start_date = 2019-03-30\n",
      "model_end_date = 2019-05-29\n",
      "fetch_start_date = 2019-03-29\n",
      "fetch_end_date = 2019-05-29\n",
      "init_start_date = 2019-03-21\n",
      "init_end_date = 2019-03-30\n"
     ]
    }
   ],
   "source": [
    "yesterday = datetime.today() - timedelta(1)\n",
    "sixtydays = yesterday - timedelta(60)\n",
    "initdate = sixtydays - timedelta(1)\n",
    "\n",
    "model_start_date = sixtydays\n",
    "model_end_date = yesterday\n",
    "\n",
    "fetch_start_date = initdate\n",
    "fetch_end_date = yesterday\n",
    "\n",
    "# The init_start_date is one day after whatever the current init_date is. Get this from the\n",
    "# file in the workspace.\n",
    "os.chdir(work_dir)\n",
    "for file in glob.glob(\"*.init\"):\n",
    "    ;\n",
    "    \n",
    "tok = file.split('_')\n",
    "foo = tok[2]\n",
    "tok = foo.split('.')\n",
    "date_str = tok[0]\n",
    "\n",
    "init_start_date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "init_end_date = sixtydays\n",
    "\n",
    "print('model_start_date = ' + model_start_date.strftime('%Y-%m-%d'))\n",
    "print('model_end_date = ' + model_end_date.strftime('%Y-%m-%d'))\n",
    "print('fetch_start_date = ' + fetch_start_date.strftime('%Y-%m-%d'))\n",
    "print('fetch_end_date = ' + fetch_end_date.strftime('%Y-%m-%d'))\n",
    "print('init_start_date = ' + init_start_date.strftime('%Y-%m-%d'))\n",
    "print('init_end_date = ' + init_end_date.strftime('%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Fetcher-Parser to bring the CBH netcdf up to yesterday. This copy from stage business simulates that Fetcher-Parser.\n",
    "\n",
    "Not implemented yet, but pull from ftp as a place holder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(work_dir)\n",
    "\n",
    "filename = 'new.nc'\n",
    "\n",
    "# Get the init file from ftp\n",
    "ftp = ftplib.FTP(ftp_server) \n",
    "ftp.login(ftp_login, ftp_pw) \n",
    "ftp.cwd(ftp_path)\n",
    "ftp.retrbinary(\"RETR \" + filename, open(filename, 'wb').write)\n",
    "ftp.quit()\n",
    "\n",
    "# shutil.copy(stage_dir + 'stage_2/new.nc', work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Using default tag: latest\\nlatest: Pulling from nhmusgs/ofp\\ne79bb959ec00: Pulling fs layer\\n2a7608a50ae3: Pulling fs layer\\ned7fdbb53a8b: Pulling fs layer\\nd9f5448383b9: Pulling fs layer\\ne902202dc40d: Pulling fs layer\\n8664813277cf: Pulling fs layer\\n6fc7615a702d: Pulling fs layer\\n7f72725e2272: Pulling fs layer\\nb4c87c3eb942: Pulling fs layer\\n0a1c8c49b2a3: Pulling fs layer\\n672c1cc6470b: Pulling fs layer\\n496a378c6ef9: Pulling fs layer\\ndac230ba3100: Pulling fs layer\\nd9f5448383b9: Waiting\\n770518b67d22: Pulling fs layer\\n8664813277cf: Waiting\\ne902202dc40d: Waiting\\n7f72725e2272: Waiting\\nb4c87c3eb942: Waiting\\n6fc7615a702d: Waiting\\n0a1c8c49b2a3: Waiting\\n672c1cc6470b: Waiting\\ndac230ba3100: Waiting\\n496a378c6ef9: Waiting\\n770518b67d22: Waiting\\ne79bb959ec00: Verifying Checksum\\ne79bb959ec00: Download complete\\n2a7608a50ae3: Verifying Checksum\\n2a7608a50ae3: Download complete\\nd9f5448383b9: Verifying Checksum\\nd9f5448383b9: Download complete\\ned7fdbb53a8b: Verifying Checksum\\ned7fdbb53a8b: Download complete\\ne902202dc40d: Download complete\\n8664813277cf: Verifying Checksum\\n8664813277cf: Download complete\\n6fc7615a702d: Verifying Checksum\\n6fc7615a702d: Download complete\\n7f72725e2272: Verifying Checksum\\n7f72725e2272: Download complete\\n0a1c8c49b2a3: Verifying Checksum\\n0a1c8c49b2a3: Download complete\\n672c1cc6470b: Download complete\\n496a378c6ef9: Verifying Checksum\\n496a378c6ef9: Download complete\\ndac230ba3100: Verifying Checksum\\ndac230ba3100: Download complete\\ne79bb959ec00: Pull complete\\n2a7608a50ae3: Pull complete\\nb4c87c3eb942: Verifying Checksum\\nb4c87c3eb942: Download complete\\n770518b67d22: Verifying Checksum\\n770518b67d22: Download complete\\ned7fdbb53a8b: Pull complete\\nd9f5448383b9: Pull complete\\ne902202dc40d: Pull complete\\n8664813277cf: Pull complete\\n6fc7615a702d: Pull complete\\n7f72725e2272: Pull complete\\nb4c87c3eb942: Pull complete\\n0a1c8c49b2a3: Pull complete\\n672c1cc6470b: Pull complete\\n496a378c6ef9: Pull complete\\ndac230ba3100: Pull complete\\n770518b67d22: Pull complete\\nDigest: sha256:17206c10b5ae9c181e4da20c249855d70fb6496a6d81d2a1fbecd8f5b6bdb5e5\\nStatus: Downloaded newer image for nhmusgs/ofp:latest\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(work_dir)\n",
    "\n",
    "# docker pull for ofp\n",
    "arg = 'docker pull nhmusgs/ofp'\n",
    "arg_l = arg.split() # need this for whatever reason; don't move arg.split into the func call below\n",
    "\n",
    "subprocess.check_output(arg_l)\n",
    "\n",
    "\n",
    "# arg = 'docker run --rm -v ' + work_dir + ':/work nhmusgs/prms:5.0.0 ' + fa + sa + ea + ia\n",
    "# arg_l = arg.split() # need this for whatever reason; don't move arg.split into the func call below\n",
    "\n",
    "# subprocess.check_output(arg_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run from onhm-runners ncf2cbh.ncf2cbh.py. This will write PRMS format CBH files that contain\n",
    "data from 2019-01-01 up through yesterday.\n",
    "\n",
    "It's here, but needs more work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure about the best way to handle importation of modules in jupyter,\n",
    "# but this seems to work\n",
    "# My PRMS python utilities are in onhm-runners/prms_utils\n",
    "if python_module_path not in sys.path:\n",
    "    sys.path.append(python_module_path)\n",
    "\n",
    "# Spell out full paths -- don't use string variables -- don't know why.\n",
    "%run '/work/markstro/operat/repos/onhm-runners/ncf2cbh/ncf2cbh.py' '/work/markstro/operat/setup/test/NHM-PRMS_CONUS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PRMS-NHM in the workspace. This run should always go from the day after the current\n",
    "init file up through yesterday.\n",
    "\n",
    "The next step works to run PRMS, but the start_time and end_time logic is not figured out yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(work_dir)\n",
    "#arg = 'docker run --rm -v ' + work_dir + ':/work prms:5.0.0 ./NHM-PRMS.control'\n",
    "\n",
    "fa = './NHM-PRMS.control -set init_vars_from_file 1 -set save_vars_to_file 0'\n",
    "\n",
    "# figure out the following from start_date and end_date\n",
    "sa = ' -set start_date ' + str(model_start_date.year) + ',' + str(model_start_date.month) + ',' + str(model_start_date.day) + ',0,0,0'\n",
    "print(sa)\n",
    "ea = ' -set end_date ' + str(model_end_date.year) + ',' + str(model_end_date.month) + ',' + str(model_end_date.day) + ',0,0,0'\n",
    "print(ea)\n",
    "ia = ' -set var_init_file prms_save_' + initdate.strftime('%Y-%m-%d') + '.init'\n",
    "print(ia)\n",
    "\n",
    "arg = 'docker run --rm -v ' + work_dir + ':/work nhmusgs/prms:5.0.0 ' + fa + sa + ea + ia\n",
    "arg_l = arg.split() # need this for whatever reason; don't move arg.split into the func call below\n",
    "\n",
    "subprocess.check_output(arg_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine if the run completed successfully and evaluate for goodness.\n",
    "\n",
    "\n",
    "This step is not yet implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the current model run was good, run prms_outputs2_ncf.py to convert the output to ncf. This will eventually run in a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure about the best way to handle importation of modules in jupyter,\n",
    "# but this seems to work\n",
    "# My PRMS python utilities are in onhm-runners/prms_utils\n",
    "\n",
    "if python_module_path not in sys.path:\n",
    "    sys.path.append(python_module_path)\n",
    "\n",
    "# Spell out full paths -- don't use string variables -- don't know why.\n",
    "%run '/work/markstro/operat/repos/onhm-runners/out2ncf/prms_outputs2_ncf.py' '/work/markstro/operat/setup/test/NHM-PRMS_CONUS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the current model run was good, run PRMS-NHM in the workspace to update the init file from whatever date it is at to yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(work_dir)\n",
    "\n",
    "# get the list of existing init files.\n",
    "old_init_file_name_list = []\n",
    "os.chdir(work_dir)\n",
    "for file in glob.glob(\"*.init\"):\n",
    "    old_init_file_name_list.append(file)\n",
    "    \n",
    "print(old_init_file_name_list)\n",
    "\n",
    "# Create the new init file\n",
    "\n",
    "fa = './NHM-PRMS.control -set init_vars_from_file 1 -set save_vars_to_file 1'\n",
    "\n",
    "# figure out the following from start_date and end_date\n",
    "sa = ' -set start_date ' + str(init_start_date.year) + ',' + str(init_start_date.month) + ',' + str(init_start_date.day) + ',0,0,0'\n",
    "print(sa)\n",
    "ea = ' -set end_date ' + str(init_end_date.year) + ',' + str(init_end_date.month) + ',' + str(init_end_date.day) + ',0,0,0'\n",
    "print(ea)\n",
    "\n",
    "ia = ' -set var_init_file prms_save_' + init_start_date.strftime('%Y-%m-%d') + '.init' + ' -set var_save_file prms_save_' + init_end_date.strftime('%Y-%m-%d') + '.init'\n",
    "print(ia)\n",
    "\n",
    "arg = 'docker run --rm -v ' + work_dir + ':/work nhmusgs/prms:5.0.0 ' + fa + sa + ea + ia\n",
    "arg_l = arg.split() # need this for whatever reason; don't move arg.split into the func call below\n",
    "\n",
    "subprocess.check_output(arg_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure that the PRMS init model run worked. If it did, erase the old init files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done for this day's step. Wait until tomorrow and go back to the date computation step and repeat subsequent steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
